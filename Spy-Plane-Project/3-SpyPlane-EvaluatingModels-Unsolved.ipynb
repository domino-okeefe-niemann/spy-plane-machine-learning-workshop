{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Models\n",
    "\n",
    "This notebook pulls in the model from the SpyPlane-RandomForestClassifier.ipynb notebook and evaluates the performance of our random forest classifier. Keep in mind that we have not yet looked into optimizing this model. \n",
    "\n",
    "This projects is based off the Buzzfeed news article on identifying spy planes found [here](https://www.buzzfeednews.com/article/peteraldhous/hidden-spy-planes), using the data and code adapted from their github repository [here](https://github.com/BuzzFeedNews/2017-08-spy-plane-finder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Follow the directions in any cell that does not contain code. If a cell does contain code, run this before moving on to the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#import packages\n",
    "\n",
    "#import packages for handling datasets and plotting\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#sci-kit learn is a library with machine learning algorithms and methods for evaluating and running these\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Train Model\n",
    "\n",
    "Repeat the data formatting and model training steps from the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This relies on output from a previous notebook!\n",
    "# If this cell does not work, try using the pregenerated data instead\n",
    "#planes_labeled = pd.read_csv(\"/mnt/data/spyplane-data/pregenerated_planes_labeled.csv\")\n",
    "planes_labeled = pd.read_csv(\"/mnt/data/spyplane-data/planes_labeled.csv\")\n",
    "\n",
    "#format data by removing non-numeric columnns and factorize the class\n",
    "X = planes_labeled.drop(['adshex','class', 'type'], axis = 1)\n",
    "y = pd.factorize(planes_labeled['class'])[0]\n",
    "\n",
    "#split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#set a random seed so results will be the same for all of us\n",
    "np.random.seed(415)\n",
    "\n",
    "#traing model on training set\n",
    "spy_model = RandomForestClassifier()\n",
    "spy_model.fit(X_train,y_train)\n",
    "\n",
    "#calculate predictions on test set\n",
    "predictions = spy_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Metrics\n",
    "\n",
    "We'll use the sci-kit learn metrics package to evaluate the model using various metrics. The documentation can be found [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy\n",
    "\n",
    "Accuracy measures the number of correct predictions divided by the total number of predictions. This method is not a good measure for cases in which the classes are imbalanced. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_true = y_test, y_pred = predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "A confusion matrix is a summary of the prediction results for a classification problem. It will show the counts of how many correct and incorrect predictions were made for both the positive and negative classes. In this example the positive class is the surveillance plane class and the negative is the other, or normal, class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the confusion matrix and create a dataframe with it to help visualize the labels\n",
    "cmtx = pd.DataFrame(\n",
    "    metrics.confusion_matrix(y_test, predictions), \n",
    "    index=['actual:other', 'actual:surveillence'], \n",
    "    columns=['predicted:other', 'predicted:surveillence']\n",
    ")\n",
    "\n",
    "cmtx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area Under the Receiver Operating Characteristic Curve (ROC AUC)\n",
    "\n",
    "The y_score can be either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions (as returned by “decision_function” on some classifiers). Here, we're using the probability estimates of the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = spy_model.predict_proba(X_test)\n",
    "y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate y_score from the predicted probabilities of the positive (spy plane) class. \n",
    "y_score = spy_model.predict_proba(X_test)[:,1]\n",
    "y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calulate false positive rate, true positive rate, and threshold\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true = y_test, y_score = y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the roc_auc area\n",
    "roc_auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the AUC-ROC\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision Score\n",
    "\n",
    "Precision shows how well our model correctly identified only the relevant surveillance plane data points. It is defined as the number of true positives divided by the sum of all of the _predicted_ positive values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall Score\n",
    "\n",
    "Recall shows how well our model correctly identified all of the relevant surveillance plane data points. It is defined as the number of true positives divided by the sum of all of the _actual_ positive values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score\n",
    "\n",
    "F1 score is the harmonic mean of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate F1 score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity\n",
    "\n",
    "Consider yourself to be a journalist using these predictions to investigate surveillance planes. What factors would influence whether you try to opmitize precision vs recall? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
